---
layout: post
title:  "机器学习笔记二"
categories: Machine_learning
tags:  总结 Machine_learning 基础知识
author: zyy
---

* content
{:toc}

本文是我在看西瓜书和机器学习教学视频的总结。




## 决策树

一般的，一棵决策树包含一个根节点、若干个内部节点和若干个叶节点。叶子结点对应于决策结果，其他每个结点则对应于一个测试属性；每个节点包含的样本集合根据属性测试的结果被划分到子结点中；根节点包含样本全集。从根节点到每个叶子节点的路径对应一个判定测试序列。决策树学习的目的是产生一个泛化能力强，即处理未见示例能力强的决策树。

决策树的生成是递归的，三种情形会导致递归返回：
1. 当前节点包含的样本全属于同一类别，无需划分；
2. 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分；
3. 当前节点包含的样本集合为空，不能划分。

决策树学习的关键是：如何选择最优划分属性。随着划分过程的不断进行，决策树的分支节点所包含的样本应尽可能属于同一类别，即节点的纯度越来越高。

### 信息熵
信息熵是度量样本集合纯度的常用指标。集合D的信息熵为Ent(D)。信息熵的值越小，纯度越高。

$$Ent(D) = - \sum_{k=1}^{|y|}p_{k}log_{2}p_{k}$$

$p_{k}$为第k类样本的比例，
$|y|$ 为样本类别总数。

### 信息增益

信息增益：使用某一属性a对某集合D进行划分之后，原集合的信息熵与各子集合D_{v}的信息熵的加权和的差值。

$$Gain(D,a)= Ent(D) - \sum_{v=1}^{V} \frac {|D_{v}|}{|D|} Ent(D_{v})$$

$\frac {|D_{v}|}{|D|}$
为第v个分支的权重，$V$为属性a的取值数。

一般来说，信息增益越大，意味着使用该属性来划分所获得的纯度提升越大。原集合的信息熵与划分后的集合的信息熵的加权和的差值越大，就说明子集合的信息熵小了许多，纯度就提升了！

因此可用信息增益来进行决策树的划分属性选择。

信息增益准则对可取值数目较多的属性有所偏好，极端的考虑：将每个样本划为一个节点，则它的纯度肯定达到最大！但是，这样的划分，没有泛化能力。

### 增益率

增益率是属性的信息增益与该属性的固有值的比值。属性的可能取值数目越多，固有值通常越大。

属性a的固有值： 
$$IV(a) = - \sum_{v=1}^{V} \frac {|D_{v}|}{|D|} log_{2} \frac {|D_{v}|}{|D|}$$

属性a的增益率： $$Gain_ratio(D,a)= \frac {Gain(D,a)} {IV(a)}$$


$\frac {|D_{v}|}{|D|}$
为第v个分支的权重，$V$为属性a的取值数。

增益率准则对可取值数目较少的属性有所偏好，因此我们在选择划分属性时，先从候选划分属性中找出信息增益高于平均水平的属性，然后再从中选择增益率最高的。

### 基尼指数

$$Gini(D) = - \sum_{k=1}^{|y|} \sum_{k^{'}!=k}p_{k}p_{k^{'}} = 1 - \sum_{k=1}^{|y|}p_{k}^{2}$$

$p_{k}$为第k类样本的比例，
$|y|$为样本类别总数。

直观来说，Gini(D)反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率。因此，Gini(D)越小，数据集的纯度越高。

属性a的基尼指数定义为：
$$Gini_index(D,a) = \sum_{v=1}^{V} \frac {|D_{v}|}{|D|} Gini(D^{v})$$

$\frac {|D_{v}|}{|D|}$
为第v个分支的权重，$V$为属性a的取值数。

使用基尼指数选择划分属性时，选择那个使得划分后基尼指数最小的属性。

## 剪枝处理

剪枝是决策树应对过拟合的主要手段，重复的节点划分造成决策树分支过多，导致过拟合。

剪枝的基本策略有：预剪枝和后剪枝。
预剪枝是在决策树生成过程中，对每个节点在划分前后先进行估计，若当前结点的划分不能带来决策树泛化性能的提升，则停止划分并将当前结点标记为子结点。
后剪枝则是先从训练集生成一颗完整的决策树，然后自底向上地对非叶子节点进行考察，若该节点对应的子树替换为叶节点能够提升决策树的泛化性能，则将该子树替换为叶节点。

所谓泛化性能的提升就是使用测试集测试，若剪枝能够提升测试集的识别准确率，则提升了泛化性能。

预剪枝使得决策树的很多分支都没有展开，不仅降低了过拟合的风险，还显著减少了决策树的训练和测试时间开销；但是，某些分支的当前划分虽不能提升泛化性能，甚至导致性能的下降，但在其基础上的后续划分，却有可能提升泛化性能，预剪枝禁止分支展开，有造成欠拟合的风险。

一般情况下，后剪枝决策树的欠拟合风险很小，泛化能力往往优于预剪枝决策树，但是后剪枝过程是在生成完全决策树之后，并且需要自地向上地对树中的所有非叶子结点进行逐一考察，因此训练时间开销比未剪枝决策树和预剪枝决策树都要大得多。


## 连续与缺失值

由于连续属性的可取值数目不再有限，需将连续属性离散化。最简单的是使用二分法进行处理。

可以对不同的候选划分点计算其信息增益，从而选择最佳的划分点。

需注意的是，与离散属性不同，若当前结点划分属性为连续属性，则该属性还可作为其后代节点的划分属性。

想要使用某些属性值缺失的样本，需要解决两个问题:
1. 如何在属性值缺失的情况下进行划分属性选择？
2. 给定划分属性，若样本在该属性上的值缺失，该如何对样本进行划分？

对于问题1，给每个样本赋一个权重，使用没有缺失值的样本配合修改的信息增益公式，计算属性的信息增益，从而选择合适的划分属性。

对于问题2，将属性值未知的样本同时划入所有子节点，并且其权重根据子节点进行调整。即让同一个样本以不同的概率划入到不同的子节点中去。

##多变量决策树

若把每个属性视为坐标空间的一个坐标轴，d个属性描述的样本就对应了d维空间的一个数据点，对样本分类则是在该坐标空间中寻找不同类别样本之间的分类边界。决策树所形成的分类边界有一个明显的特点：轴平行。即它的分类边界由若干个与坐标轴平行的分段组成。当学习任务较复杂时，必须使用很多段划分才能获得较好的结果，时间开销较大。

多变量决策树是能够实现斜划分或者更加复杂划分的决策树。其非叶子结点不再仅对某一个属性，而是对属性的线性组合进行测试，每个属性一个权重，形成一个线性方程，再根据其值进行划分。多变量决策树在学习过程中，不是为每一个非叶节点寻找一个最优划分属性，而是试图建立一个合适的线型分类器。




